{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "1a4769e8-4998-4bb3-8b82-5dcfa7c4f3f7",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "58b17518-67ae-4a4c-82d5-aeb747be2b26",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'pd' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-1-80b900fad69f>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[0mdf\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mpd\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mread_csv\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'aug_train.csv'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      2\u001b[0m \u001b[0mdf\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mNameError\u001b[0m: name 'pd' is not defined"
     ]
    }
   ],
   "source": [
    "df = pd.read_csv('aug_train.csv')\n",
    "df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bb1e35ae-8d92-4bea-94f3-bb1dd89a6d3f",
   "metadata": {},
   "source": [
    "# Opening a csv file from an URL"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "31b315bf-f2ce-4382-ae43-4f162b498277",
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "from io import StringIO\n",
    "\n",
    "url = \"https://raw.githubusercontent.com/cs109/2014_data/master/countries.csv\"\n",
    "headers = {\"User-Agent\": \"Mozilla/5.0 (Macintosh; Intel Mac OS X 10.14; rv:66.0) Gecko/20100101 Firefox/66.0\"}\n",
    "req = requests.get(url, headers=headers)\n",
    "data = StringIO(req.text)\n",
    "\n",
    "pd.read_csv(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1e534017-6c9f-4805-b651-ad601ec5885e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# separated by tab, to open tsv file\n",
    "pd.read_csv('movie_titles_metadata.tsv',sep='\\t',names=['sno','name','release_year','rating','votes','genres'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2395d385-52a5-4716-93aa-e98b8a980fea",
   "metadata": {},
   "outputs": [],
   "source": [
    "# starts the indexing with the provided column name\n",
    "pd.read_csv('aug_train.csv',index_col='enrollee_id') "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8d21009c-cc2d-44be-8782-3ec96646531a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# if the columns names is in the first row then this parameter can be used to make the first row to the column name\n",
    "pd.read_csv('test.csv',header=1) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "046d16ea-4518-4a72-9d3b-dcb945a63b61",
   "metadata": {},
   "outputs": [],
   "source": [
    "# This parameter is only extracting the given columns form the whole dataset\n",
    "pd.read_csv('aug_train.csv',usecols=['enrollee_id','gender','education_level'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "831b8f0c-0911-4458-b529-16f0717f798a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# This parameter is used to extract the specfic number of rows form any datasets, below only 100 rows are considered\n",
    "pd.read_csv('aug_train.csv',nrows=100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5eb4bb1f-dc40-4e9f-9f5a-47a0a909e7b5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# This parameter is used when the encoding of the csv file is different than UTF-8 or RFC-4180. On that condition either finds its encoding and define\n",
    "# like below or change the encoding from sublime text\n",
    "pd.read_csv('zomato.csv',encoding='latin-1')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bf7e527d-3278-4d78-a32d-445c3da5177a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# This parameter is used to skip the rows which has values more than expected values. Suppose a dataset has 8 columns This parameter filters out the\n",
    "# rows with more than 8 values and removes it\n",
    "pd.read_csv('BX-Books.csv', sep=';', encoding=\"latin-1\",error_bad_lines=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "27da2ed8-7b08-4b9c-942a-1c2f35dfb455",
   "metadata": {},
   "outputs": [],
   "source": [
    "# This parameter directyly changes the dtype of the given column to the given type as below\n",
    "pd.read_csv('aug_train.csv',dtype={'target':int}).info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "417101f2-e7c4-4231-b28d-92f0101c8f8e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# changing the colums type to date now this is not just a regular column but a date column where certain operations can be performed\n",
    "pd.read_csv('IPL Matches 2008-2020.csv',parse_dates=['date']).info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8e0d2640-c4d7-4202-87af-4b7efe8dfd65",
   "metadata": {},
   "outputs": [],
   "source": [
    "# This parameter is used to change something inside the dataset. below  Royal Challengers Bangalore is changed to RCB. Now, inside the data everything\n",
    "# is changed to RCB\n",
    "def rename(name):\n",
    "    if name == \"Royal Challengers Bangalore\":\n",
    "        return \"RCB\"\n",
    "    else:\n",
    "        return name\n",
    "rename(\"Royal Challengers Bangalore\")\n",
    "pd.read_csv('IPL Matches 2008-2020.csv',converters={'team1':rename})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7c09f4a5-623c-40dd-a6ef-7d5e34c329fa",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Dviding a large dataset to smaller chunks so that the memory load is reduced, supoose the dataset is 20000 here the dataset is decomposed to 5000 of\n",
    "# 4 equal sets\n",
    "dfs = pd.read_csv('aug_train.csv',chunksize=5000)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7ff34fb0-12d1-43ff-9031-bbd38acb4afe",
   "metadata": {},
   "source": [
    "# Handling JSON Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2689aad0-c679-4c65-9017-5a4285d7d662",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Loadint the file from same folder\n",
    "df = pd.read_json(\"file_name_here\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f33c685a-0265-4877-859d-0428ebea38fc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Loadin json form url\n",
    "df = pd.read_json(\"paste_link_here\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
