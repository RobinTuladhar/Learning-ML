{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "cf72521e",
   "metadata": {},
   "source": [
    "# Mini-Batch Gradient Descent (MBGD)\n",
    "is a variation of the Gradient Descent optimization algorithm that strikes a balance between the computational efficiency of Stochastic Gradient Descent (SGD) and the stability of Batch Gradient Descent (BGD). It's a popular choice in machine learning and deep learning because it combines the benefits of both methods.Stochastic gradient descent (SGD) is a machine learning algorithm for optimizing an objective function. It is a type of gradient descent, which means that it follows the gradient of the objective function to find the minimum. However, SGD does not use the entire dataset to calculate the gradient at each step. Instead, it uses a single data point or a small subset of data points, called a minibatch. This makes SGD much faster than traditional gradient descent, which can be computationally expensive for large datasets.\n",
    "\n",
    "Here is an analogy to help you understand SGD. Imagine you are lost in a forest and you want to find the shortest path to the exit. You could start by walking in a straight line, but you would probably end up going in circles. A better way would be to follow a trail. The trail will lead you in the right direction, even if it is not the shortest path. SGD is like following a trail in the forest. It is not the shortest path to the minimum, but it is a much faster way to get there.\n",
    "\n",
    "## Objective:\n",
    "Like other gradient-based algorithms, MBGD aims to update a model's parameters to minimize a cost function. However, it does so by considering small, random subsets of the training data at each iteration, rather than the entire dataset.\n",
    "\n",
    "Explanation:\n",
    "\n",
    "1. Data Batches: In MBGD, you divide your entire training dataset into smaller, equally-sized chunks called \"mini-batches.\" These mini-batches typically contain a few dozen to a few hundred data points.\n",
    "\n",
    "2. Gradient Calculation: At each iteration (or epoch), MBGD computes the gradient of the cost function using one of these mini-batches. This means it updates the model's parameters based on a subset of the data, not the entire dataset.\n",
    "\n",
    "3. Parameter Update: MBGD then adjusts the model's parameters using the computed gradient. The update is done similarly to BGD, but it's based on the mini-batch gradient rather than the full dataset gradient.\n",
    "\n",
    "## Advantage\n",
    "1. Faster Convergence: MBGD often converges faster than BGD because it updates the parameters more frequently. This speedup can be particularly significant for large datasets.\n",
    "\n",
    "2. Efficient Memory Usage: Unlike BGD, which requires storing the entire dataset in memory, MBGD only needs to load one mini-batch at a time, making it memory-efficient.\n",
    "\n",
    "3. Better Generalization: Mini-batches introduce a level of randomness into the optimization process, which can help the model generalize better and avoid getting stuck in local minima.\n",
    "4. It is much faster than traditional gradient descent for large datasets.\n",
    "5. is relatively easy to implement.\n",
    "6. It can be used to train a wide variety of machine learning models.\n",
    "\n",
    "## Disadvantage\n",
    "\n",
    "1. Learning Rate Tuning: Choosing an appropriate learning rate for MBGD can be trickier than for BGD. It may require some trial and error to find the right learning rate.\n",
    "\n",
    "2. Less Stable Convergence: While MBGD is faster, it can have more oscillations in the optimization path compared to BGD due to the randomness introduced by mini-batches.\n",
    "\n",
    "3. It can be less accurate than traditional gradient descent.\n",
    "4. It can be more susceptible to getting stuck in local minima.\n",
    "5. It can require more tuning of hyperparameters.\n",
    "\n",
    "## Common Applications:\n",
    "\n",
    "1. Deep Learning: MBGD is widely used for training deep neural networks, where large datasets are common, and computational efficiency is crucial.\n",
    "\n",
    "2. Natural Language Processing: It's employed in tasks like text classification and language modeling where datasets can be large.\n",
    "\n",
    "3. Computer Vision: MBGD is applied to tasks such as image classification and object detection.\n",
    "4. Training neural networks\n",
    "5. Linear regression\n",
    "6. Logistic regression\n",
    "7. Support vector machines\n",
    "8. Natural language processing\n",
    "9. Image classification\n",
    "10. Speech recognition\n",
    "\n",
    "In essence, Mini-Batch Gradient Descent combines the advantages of both Batch and Stochastic Gradient Descent. It's a versatile optimization method used in various machine learning applications, offering faster convergence and efficient memory usage while maintaining good generalization performance."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "40b09bc8",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.datasets import load_diabetes\n",
    "\n",
    "import numpy as np\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.metrics import r2_score\n",
    "from sklearn.model_selection import train_test_split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "7a965b7d",
   "metadata": {},
   "outputs": [],
   "source": [
    "X,y = load_diabetes(return_X_y=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "ee147f58",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(442, 10)\n",
      "(442,)\n"
     ]
    }
   ],
   "source": [
    "print(X.shape)\n",
    "print(y.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "b7a87398",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(X,y, test_size=0.2, random_state=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "89880a3b",
   "metadata": {},
   "outputs": [],
   "source": [
    "reg = LinearRegression()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "7e3a6dce",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<style>#sk-container-id-1 {color: black;background-color: white;}#sk-container-id-1 pre{padding: 0;}#sk-container-id-1 div.sk-toggleable {background-color: white;}#sk-container-id-1 label.sk-toggleable__label {cursor: pointer;display: block;width: 100%;margin-bottom: 0;padding: 0.3em;box-sizing: border-box;text-align: center;}#sk-container-id-1 label.sk-toggleable__label-arrow:before {content: \"▸\";float: left;margin-right: 0.25em;color: #696969;}#sk-container-id-1 label.sk-toggleable__label-arrow:hover:before {color: black;}#sk-container-id-1 div.sk-estimator:hover label.sk-toggleable__label-arrow:before {color: black;}#sk-container-id-1 div.sk-toggleable__content {max-height: 0;max-width: 0;overflow: hidden;text-align: left;background-color: #f0f8ff;}#sk-container-id-1 div.sk-toggleable__content pre {margin: 0.2em;color: black;border-radius: 0.25em;background-color: #f0f8ff;}#sk-container-id-1 input.sk-toggleable__control:checked~div.sk-toggleable__content {max-height: 200px;max-width: 100%;overflow: auto;}#sk-container-id-1 input.sk-toggleable__control:checked~label.sk-toggleable__label-arrow:before {content: \"▾\";}#sk-container-id-1 div.sk-estimator input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-1 div.sk-label input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-1 input.sk-hidden--visually {border: 0;clip: rect(1px 1px 1px 1px);clip: rect(1px, 1px, 1px, 1px);height: 1px;margin: -1px;overflow: hidden;padding: 0;position: absolute;width: 1px;}#sk-container-id-1 div.sk-estimator {font-family: monospace;background-color: #f0f8ff;border: 1px dotted black;border-radius: 0.25em;box-sizing: border-box;margin-bottom: 0.5em;}#sk-container-id-1 div.sk-estimator:hover {background-color: #d4ebff;}#sk-container-id-1 div.sk-parallel-item::after {content: \"\";width: 100%;border-bottom: 1px solid gray;flex-grow: 1;}#sk-container-id-1 div.sk-label:hover label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-1 div.sk-serial::before {content: \"\";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: 0;}#sk-container-id-1 div.sk-serial {display: flex;flex-direction: column;align-items: center;background-color: white;padding-right: 0.2em;padding-left: 0.2em;position: relative;}#sk-container-id-1 div.sk-item {position: relative;z-index: 1;}#sk-container-id-1 div.sk-parallel {display: flex;align-items: stretch;justify-content: center;background-color: white;position: relative;}#sk-container-id-1 div.sk-item::before, #sk-container-id-1 div.sk-parallel-item::before {content: \"\";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: -1;}#sk-container-id-1 div.sk-parallel-item {display: flex;flex-direction: column;z-index: 1;position: relative;background-color: white;}#sk-container-id-1 div.sk-parallel-item:first-child::after {align-self: flex-end;width: 50%;}#sk-container-id-1 div.sk-parallel-item:last-child::after {align-self: flex-start;width: 50%;}#sk-container-id-1 div.sk-parallel-item:only-child::after {width: 0;}#sk-container-id-1 div.sk-dashed-wrapped {border: 1px dashed gray;margin: 0 0.4em 0.5em 0.4em;box-sizing: border-box;padding-bottom: 0.4em;background-color: white;}#sk-container-id-1 div.sk-label label {font-family: monospace;font-weight: bold;display: inline-block;line-height: 1.2em;}#sk-container-id-1 div.sk-label-container {text-align: center;}#sk-container-id-1 div.sk-container {/* jupyter's `normalize.less` sets `[hidden] { display: none; }` but bootstrap.min.css set `[hidden] { display: none !important; }` so we also need the `!important` here to be able to override the default hidden behavior on the sphinx rendered scikit-learn.org. See: https://github.com/scikit-learn/scikit-learn/issues/21755 */display: inline-block !important;position: relative;}#sk-container-id-1 div.sk-text-repr-fallback {display: none;}</style><div id=\"sk-container-id-1\" class=\"sk-top-container\"><div class=\"sk-text-repr-fallback\"><pre>LinearRegression()</pre><b>In a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. <br />On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.</b></div><div class=\"sk-container\" hidden><div class=\"sk-item\"><div class=\"sk-estimator sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-1\" type=\"checkbox\" checked><label for=\"sk-estimator-id-1\" class=\"sk-toggleable__label sk-toggleable__label-arrow\">LinearRegression</label><div class=\"sk-toggleable__content\"><pre>LinearRegression()</pre></div></div></div></div></div>"
      ],
      "text/plain": [
       "LinearRegression()"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "reg.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "753cc81c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[  -9.15865318 -205.45432163  516.69374454  340.61999905 -895.5520019\n",
      "  561.22067904  153.89310954  126.73139688  861.12700152   52.42112238]\n",
      "151.88331005254167\n"
     ]
    }
   ],
   "source": [
    "print(reg.coef_)\n",
    "print(reg.intercept_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "ff9ac4c9",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.4399338661568968"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_pred = reg.predict(X_test)\n",
    "r2_score(y_test, y_pred)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "06f67cf4",
   "metadata": {},
   "source": [
    "# Making my own class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "d5030c59",
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "\n",
    "class MBGDRegressor:\n",
    "    \n",
    "    def __init__(self,batch_size,learning_rate=0.01,epochs=100):\n",
    "        \n",
    "        self.coef_ = None\n",
    "        self.intercept_ = None\n",
    "        self.lr = learning_rate\n",
    "        self.epochs = epochs\n",
    "        self.batch_size = batch_size\n",
    "        \n",
    "    def fit(self,X_train,y_train):\n",
    "        # init your coefs\n",
    "        self.intercept_ = 0\n",
    "        self.coef_ = np.ones(X_train.shape[1])\n",
    "        \n",
    "        for i in range(self.epochs):\n",
    "            \n",
    "            for j in range(int(X_train.shape[0]/self.batch_size)):\n",
    "                \n",
    "                idx = random.sample(range(X_train.shape[0]),self.batch_size)\n",
    "                \n",
    "                y_hat = np.dot(X_train[idx],self.coef_) + self.intercept_\n",
    "                #print(\"Shape of y_hat\",y_hat.shape)\n",
    "                intercept_der = -2 * np.mean(y_train[idx] - y_hat)\n",
    "                self.intercept_ = self.intercept_ - (self.lr * intercept_der)\n",
    "\n",
    "                coef_der = -2 * np.dot((y_train[idx] - y_hat),X_train[idx])\n",
    "                self.coef_ = self.coef_ - (self.lr * coef_der)\n",
    "        \n",
    "        print(self.intercept_,self.coef_)\n",
    "    \n",
    "    def predict(self,X_test):\n",
    "        return np.dot(X_test,self.coef_) + self.intercept_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "85f0723e",
   "metadata": {},
   "outputs": [],
   "source": [
    "mbr = MBGDRegressor(batch_size=int(X_train.shape[0]/50),learning_rate=0.01,epochs=100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "665f2fd7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "149.05038634411366 [  35.90625821 -135.96077789  442.42252856  294.45932664  -21.73197737\n",
      "  -88.18605825 -185.84746272  106.59877843  397.96723993  115.92261399]\n"
     ]
    }
   ],
   "source": [
    "mbr.fit(X_train,y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "b0fe3ab9",
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred = mbr.predict(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "5badbc05",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.45088457983308416"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "r2_score(y_test,y_pred)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9eb3e4de",
   "metadata": {},
   "source": [
    "# Using sklearn\n",
    "Batch size in sgdregressor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "937cc586",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.linear_model import SGDRegressor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "a344b257",
   "metadata": {},
   "outputs": [],
   "source": [
    "sgd = SGDRegressor(learning_rate='constant',eta0=0.1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "71108cd5",
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 35\n",
    "\n",
    "for i in range(100):\n",
    "    \n",
    "    idx = random.sample(range(X_train.shape[0]),batch_size)\n",
    "    sgd.partial_fit(X_train[idx],y_train[idx])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "e372b48d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([  58.9614017 ,  -70.02031461,  346.88463031,  250.06957973,\n",
       "          1.13474738,  -40.4056732 , -185.27398386,  123.86660869,\n",
       "        310.15432892,  120.53006075])"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sgd.coef_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "bf6fe35e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([161.80410321])"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sgd.intercept_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "1702f663",
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred = sgd.predict(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "3eb582b9",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.4201702669239188"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "r2_score(y_test,y_pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7919b855",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
