{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "f8408f9a",
   "metadata": {},
   "source": [
    "### Batch Gradient Descent (BGD), often referred to simply as Gradient Descent, is an optimization algorithm used in machine learning and deep learning to minimize the cost or loss function of a model during training. It is a fundamental technique for updating the parameters of a model in order to find the best possible set of parameters that minimize the error between the model's predictions and the actual target values.\n",
    "\n",
    "\n",
    "## Advantage\n",
    "1. Stable Convergence: BGD typically converges to a global minimum (or a good local minimum) if the cost function is convex.\n",
    "\n",
    "2. Precise Gradient Estimate: Using the entire dataset provides an accurate estimate of the gradient, reducing the chances of noisy updates.\n",
    "\n",
    "3. It is a simple and easy to understand algorithm.\n",
    "\n",
    "4. It can be used to minimize any cost function, regardless of its shape.\n",
    "\n",
    "5. It can be used to train models with a large number of parameters.\n",
    "\n",
    "## Disadvantage\n",
    "\n",
    "1. Computationally Intensive: BGD requires processing the entire training dataset at each iteration, making it computationally expensive, especially for large datasets.\n",
    "\n",
    "2. Memory Usage: It may not be feasible to load the entire dataset into memory, leading to memory constraints for very large datasets.\n",
    "\n",
    "3. Slower Convergence: BGD often converges more slowly than other variants of gradient descent due to infrequent parameter updates.\n",
    "\n",
    "4. It can be slow to converge, especially for large datasets.\n",
    "\n",
    "5. It can be sensitive to the choice of the learning rate.\n",
    "\n",
    "### Batch Gradient Descent (BGD) finds its application across various machine learning domains, including linear regression, logistic regression, and neural networks. It stands out as a reliable optimization method, particularly for scenarios with manageable dataset sizes. Let's consolidate the key points about BGD:\n",
    "\n",
    "1. Linear Regression: BGD is frequently employed to train linear regression models, where its stability and well-defined convergence are advantageous.\n",
    "\n",
    "2. Deep Learning: In the initial training phases of neural networks, BGD is a valuable choice, especially when the dataset can comfortably fit into memory. It provides a solid foundation for optimizing neural network parameters.\n",
    "\n",
    "3. Convex Optimization: BGD shines in problems featuring convex cost functions. Its suitability for such scenarios makes it a preferred optimization technique.\n",
    "\n",
    "To summarize, Batch Gradient Descent is a gradient-based optimization method that iteratively adjusts model parameters using gradients computed from the entire training dataset. Its strength lies in delivering robust convergence, but it comes with computational and speed trade-offs, particularly when dealing with extensive datasets. Its applicability spans linear regression, neural network training, and convex optimization problems.\n",
    "\n",
    "Additional considerations include fine-tuning the learning rate, a crucial hyperparameter influencing the algorithm's convergence speed. Smaller learning rates lead to slower but more stable convergence, while larger ones can risk divergence. The choice of the number of epochs, representing passes through the dataset, impacts both model accuracy and training time. Furthermore, BGD can be combined with various optimization techniques like momentum and adaptive learning rates to enhance its convergence and accuracy in practice."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "33219a55",
   "metadata": {},
   "source": [
    "#### Fetching the actual coef and intercept values for this dataset using the linear regresiion model from skelarn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "d915683e",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.datasets import load_diabetes\n",
    "\n",
    "import numpy as np\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.metrics import r2_score\n",
    "from sklearn.model_selection import train_test_split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "d616023c",
   "metadata": {},
   "outputs": [],
   "source": [
    "X,y = load_diabetes(return_X_y=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "2e8f3a3a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(442, 10)\n",
      "(442,)\n"
     ]
    }
   ],
   "source": [
    "print(X.shape)\n",
    "print(y.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "95cffd6c",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(X,y, test_size=0.2,random_state=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "d6d1126b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<style>#sk-container-id-1 {color: black;background-color: white;}#sk-container-id-1 pre{padding: 0;}#sk-container-id-1 div.sk-toggleable {background-color: white;}#sk-container-id-1 label.sk-toggleable__label {cursor: pointer;display: block;width: 100%;margin-bottom: 0;padding: 0.3em;box-sizing: border-box;text-align: center;}#sk-container-id-1 label.sk-toggleable__label-arrow:before {content: \"▸\";float: left;margin-right: 0.25em;color: #696969;}#sk-container-id-1 label.sk-toggleable__label-arrow:hover:before {color: black;}#sk-container-id-1 div.sk-estimator:hover label.sk-toggleable__label-arrow:before {color: black;}#sk-container-id-1 div.sk-toggleable__content {max-height: 0;max-width: 0;overflow: hidden;text-align: left;background-color: #f0f8ff;}#sk-container-id-1 div.sk-toggleable__content pre {margin: 0.2em;color: black;border-radius: 0.25em;background-color: #f0f8ff;}#sk-container-id-1 input.sk-toggleable__control:checked~div.sk-toggleable__content {max-height: 200px;max-width: 100%;overflow: auto;}#sk-container-id-1 input.sk-toggleable__control:checked~label.sk-toggleable__label-arrow:before {content: \"▾\";}#sk-container-id-1 div.sk-estimator input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-1 div.sk-label input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-1 input.sk-hidden--visually {border: 0;clip: rect(1px 1px 1px 1px);clip: rect(1px, 1px, 1px, 1px);height: 1px;margin: -1px;overflow: hidden;padding: 0;position: absolute;width: 1px;}#sk-container-id-1 div.sk-estimator {font-family: monospace;background-color: #f0f8ff;border: 1px dotted black;border-radius: 0.25em;box-sizing: border-box;margin-bottom: 0.5em;}#sk-container-id-1 div.sk-estimator:hover {background-color: #d4ebff;}#sk-container-id-1 div.sk-parallel-item::after {content: \"\";width: 100%;border-bottom: 1px solid gray;flex-grow: 1;}#sk-container-id-1 div.sk-label:hover label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-1 div.sk-serial::before {content: \"\";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: 0;}#sk-container-id-1 div.sk-serial {display: flex;flex-direction: column;align-items: center;background-color: white;padding-right: 0.2em;padding-left: 0.2em;position: relative;}#sk-container-id-1 div.sk-item {position: relative;z-index: 1;}#sk-container-id-1 div.sk-parallel {display: flex;align-items: stretch;justify-content: center;background-color: white;position: relative;}#sk-container-id-1 div.sk-item::before, #sk-container-id-1 div.sk-parallel-item::before {content: \"\";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: -1;}#sk-container-id-1 div.sk-parallel-item {display: flex;flex-direction: column;z-index: 1;position: relative;background-color: white;}#sk-container-id-1 div.sk-parallel-item:first-child::after {align-self: flex-end;width: 50%;}#sk-container-id-1 div.sk-parallel-item:last-child::after {align-self: flex-start;width: 50%;}#sk-container-id-1 div.sk-parallel-item:only-child::after {width: 0;}#sk-container-id-1 div.sk-dashed-wrapped {border: 1px dashed gray;margin: 0 0.4em 0.5em 0.4em;box-sizing: border-box;padding-bottom: 0.4em;background-color: white;}#sk-container-id-1 div.sk-label label {font-family: monospace;font-weight: bold;display: inline-block;line-height: 1.2em;}#sk-container-id-1 div.sk-label-container {text-align: center;}#sk-container-id-1 div.sk-container {/* jupyter's `normalize.less` sets `[hidden] { display: none; }` but bootstrap.min.css set `[hidden] { display: none !important; }` so we also need the `!important` here to be able to override the default hidden behavior on the sphinx rendered scikit-learn.org. See: https://github.com/scikit-learn/scikit-learn/issues/21755 */display: inline-block !important;position: relative;}#sk-container-id-1 div.sk-text-repr-fallback {display: none;}</style><div id=\"sk-container-id-1\" class=\"sk-top-container\"><div class=\"sk-text-repr-fallback\"><pre>LinearRegression()</pre><b>In a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. <br />On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.</b></div><div class=\"sk-container\" hidden><div class=\"sk-item\"><div class=\"sk-estimator sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-1\" type=\"checkbox\" checked><label for=\"sk-estimator-id-1\" class=\"sk-toggleable__label sk-toggleable__label-arrow\">LinearRegression</label><div class=\"sk-toggleable__content\"><pre>LinearRegression()</pre></div></div></div></div></div>"
      ],
      "text/plain": [
       "LinearRegression()"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "reg = LinearRegression()\n",
    "reg.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "869af185",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.4399338661568968"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_pred = reg.predict(X_test)\n",
    "r2_score(y_test, y_pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "1601cc28",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[  -9.15865318 -205.45432163  516.69374454  340.61999905 -895.5520019\n",
      "  561.22067904  153.89310954  126.73139688  861.12700152   52.42112238]\n",
      "151.88331005254167\n"
     ]
    }
   ],
   "source": [
    "print(reg.coef_) # 10 columns 10 coefficients \n",
    "print(reg.intercept_) # and one intercept"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ad1f3a85",
   "metadata": {},
   "source": [
    "# Making my own class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "232aba52",
   "metadata": {},
   "outputs": [],
   "source": [
    "class GDRegressor:\n",
    "    \n",
    "    def __init__(self,learning_rate=0.01,epochs=100):\n",
    "        \n",
    "        self.coef_ = None\n",
    "        self.intercept_ = None\n",
    "        self.lr = learning_rate\n",
    "        self.epochs = epochs\n",
    "        \n",
    "    def fit(self,X_train,y_train):\n",
    "        # init your coefs\n",
    "        self.intercept_ = 0\n",
    "        self.coef_ = np.ones(X_train.shape[1])\n",
    "        \n",
    "        for i in range(self.epochs):\n",
    "            # update all the coef and the intercept\n",
    "            y_hat = np.dot(X_train,self.coef_) + self.intercept_\n",
    "            intercept_der = -2 * np.mean(y_train - y_hat)\n",
    "            self.intercept_ = self.intercept_ - (self.lr * intercept_der)\n",
    "            \n",
    "            coef_der = -2 * np.dot((y_train - y_hat),X_train)/X_train.shape[0]\n",
    "            self.coef_ = self.coef_ - (self.lr * coef_der)\n",
    "        \n",
    "        print(self.intercept_,self.coef_)\n",
    "    \n",
    "    def predict(self,X_test):\n",
    "        return np.dot(X_test,self.coef_) + self.intercept_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "61e3412e",
   "metadata": {},
   "outputs": [],
   "source": [
    "gdr = GDRegressor(epochs=1000, learning_rate=\n",
    "                 0.5) # making object of the class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "181c48ed",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "152.01351687661833 [  14.38990585 -173.7235727   491.54898524  323.91524824  -39.32648042\n",
      " -116.01061213 -194.04077415  103.38135565  451.63448787   97.57218278]\n"
     ]
    }
   ],
   "source": [
    "gdr.fit(X_train, y_train) # intercept and coef terms"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "7508bff6",
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred = gdr.predict(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "b50cf947",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.4534503034722803"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "r2_score(y_test, y_pred)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e46738c0",
   "metadata": {},
   "source": [
    "# Normal Gradient descent is batch gradient descent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d225b647",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
