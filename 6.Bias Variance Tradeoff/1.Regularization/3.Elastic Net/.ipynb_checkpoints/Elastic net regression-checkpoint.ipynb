{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "7185ba50",
   "metadata": {},
   "source": [
    "# Elastic Net\n",
    "Elastic Net is a powerful regularization technique in machine learning, especially effective when dealing with datasets that contain many features and potential collinearity (correlations between features). It combines the strengths of both ridge regression and lasso regularization. Elastic Net achieves this by adding a penalty to the model's objective function, which is a mix of the sum of squared coefficients (like ridge regression) and the sum of absolute values of coefficients (like lasso regularization). The balance between these two penalties is controlled by a parameter called \"alpha.\" When alpha is set to zero, Elastic Net behaves like ridge regression, while an alpha of one makes it act like lasso regularization. Values of alpha between zero and one give you the versatility of Elastic Net, providing you with a flexible tool to manage complex datasets and enhance predictive model performance.\n",
    "\n",
    "## Explanation:\n",
    "Imagine you're building a model to predict something, like a person's income, based on various factors such as education, age, and work experience. You have many features, and some of them may be closely related to each other, making it challenging for your model to decide which features to focus on and how much importance to assign to each.\n",
    "\n",
    "Elastic Net comes to the rescue with a unique approach:\n",
    "\n",
    "Balancing Act: Elastic Net adds a special term to the model's learning process. This term combines both Ridge (L2) and Lasso (L1) penalties. Like Ridge, it encourages the model to use all the features to some extent and like Lasso, it encourages some features to have exactly zero coefficients, effectively performing feature selection.\n",
    "\n",
    "## Advantages\n",
    "1. Flexibility: Elastic Net is flexible because it allows you to control the balance between Ridge and Lasso regularization. You can tune a hyperparameter to emphasize one regularization technique over the other, depending on your data and problem.\n",
    "2. Collinearity Handling: It handles multicollinearity (correlations between features) well, which can be a challenge for Ridge and Lasso individually.\n",
    "3. Feature Selection: Like Lasso, Elastic Net can automatically select important features and set less important ones to zero\n",
    "4. It can help to prevent overfitting.\n",
    "5. It can improve the generalization performance of the model.\n",
    "6. It can make the model more interpretable.\n",
    "7. It can select features, which means that it can identify the features that are most important for predicting the target variable.\n",
    "\n",
    "## Disadvantages\n",
    "\n",
    "1. Hyperparameter Tuning: Elastic Net has an extra hyperparameter to tune compared to Ridge and Lasso, making it slightly more complex to use.\n",
    "2. Interpretability: While it simplifies models by setting some feature coefficients to zero, it may not result in models that are as interpretable as simpler models without regularization.\n",
    "3. It can reduce the accuracy of the model on the training data.\n",
    "4. It can be computationally expensive to train the model.\n",
    "5. It can be more sensitive to the choice of hyperparameters.\n",
    "\n",
    "## Application and Uses\n",
    "\n",
    "Elastic Net is widely used in various machine learning domains, including:\n",
    "\n",
    "1. Predicting Financial Outcomes: For forecasting stock prices, credit risk assessment, and portfolio optimization.\n",
    "2. Healthcare: In predicting patient outcomes or diagnosing diseases based on medical data.\n",
    "3. Natural Language Processing: For text classification, sentiment analysis, and topic modeling.\n",
    "\n",
    "In summary, Elastic Net is like a wise negotiator that balances the interests of all your features, ensuring they contribute appropriately to your machine learning model. It offers flexibility, handles collinearity, and performs feature selection, making it a valuable tool in situations where complex and feature-rich data need to be tamed for accurate predictions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "id": "71e28874",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.datasets import load_diabetes\n",
    "from sklearn.linear_model import LinearRegression,Ridge,Lasso,ElasticNet\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import r2_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "id": "284ccf3c",
   "metadata": {},
   "outputs": [],
   "source": [
    "X,y = load_diabetes(return_X_y=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "id": "799d9080",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train,X_test,y_train,y_test = train_test_split(X,y,test_size=0.2,random_state=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "id": "8a242b9f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.4399387660024645"
      ]
     },
     "execution_count": 73,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Linear Regression\n",
    "reg = LinearRegression()\n",
    "reg.fit(X_train,y_train)\n",
    "y_pred = reg.predict(X_test)\n",
    "r2_score(y_test,y_pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "id": "a41eb10c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.4519973816947852"
      ]
     },
     "execution_count": 74,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Ridge \n",
    "reg = Ridge(alpha=0.1)\n",
    "reg.fit(X_train,y_train)\n",
    "y_pred = reg.predict(X_test)\n",
    "r2_score(y_test,y_pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "id": "b6b53e7c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.4411227990495632"
      ]
     },
     "execution_count": 75,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Lasso\n",
    "reg = Lasso(alpha=0.01)\n",
    "reg.fit(X_train,y_train)\n",
    "y_pred = reg.predict(X_test)\n",
    "r2_score(y_test,y_pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "id": "5fc65b12",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.4531493801165679"
      ]
     },
     "execution_count": 76,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# ElasticNet\n",
    "reg = ElasticNet(alpha=0.005,l1_ratio=0.9)\n",
    "reg.fit(X_train,y_train)\n",
    "y_pred = reg.predict(X_test)\n",
    "r2_score(y_test,y_pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0cb67b0c",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
